{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdb806e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    pipeline,\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import getpass\n",
    "import openai # For GPT-4o Zero-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f17d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMN = \"cEXT\" \n",
    "TEXT_COLUMN = \"STATUS\"\n",
    "DATA_FILE = \"/data/jmharja/projects/PersonaClassifier/data/mypersonality.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13df1e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(file_path, text_col, target_col):\n",
    "    \"\"\"Loads data from a CSV and prepares it for all experiments.\"\"\"\n",
    "    print(\"Loading and preparing data...\")\n",
    "    df = pd.read_csv(file_path, encoding='Windows-1252')\n",
    "    df = df.dropna(subset=[text_col, target_col])\n",
    "    df['label'] = df[target_col].apply(lambda x: 1 if str(x).lower() == 'y' else 0)\n",
    "    df_processed = df[[text_col, 'label']].rename(columns={text_col: 'text'})\n",
    "    \n",
    "    train_df, test_df = train_test_split(df_processed, test_size=0.2, random_state=42, stratify=df_processed['label'])\n",
    "    \n",
    "    # For Hugging Face models\n",
    "    train_val_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
    "    test_dataset = Dataset.from_pandas(test_df, preserve_index=False)\n",
    "    \n",
    "    train_val_split = train_val_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    \n",
    "    dataset_dict = DatasetDict({\n",
    "        'train': train_val_split['train'],\n",
    "        'validation': train_val_split['test'],\n",
    "        'test': test_dataset\n",
    "    })\n",
    "    \n",
    "    print(\"Data preparation complete.\")\n",
    "    return dataset_dict, train_df, test_df\n",
    "\n",
    "# Load data once for all experiments\n",
    "main_dataset_dict, train_df, test_df = load_and_prepare_data(DATA_FILE, TEXT_COLUMN, TARGET_COLUMN)\n",
    "print(\"\\nDataset structure for Transformer models:\")\n",
    "print(main_dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e0b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import whoami\n",
    "print(\"HEHEHe\")\n",
    "try:\n",
    "    user_info = whoami()\n",
    "    print(\"Authenticated as:\", user_info['name'])\n",
    "except Exception as e:\n",
    "    print(\"Not logged in or token is invalid.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70466724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "hf_token = os.environ.get(\"HUGGING_FACE_HUB_TOKEN\") or os.environ.get(\"HF_TOKEN\")\n",
    "if hf_token:\n",
    "    print(\"An environment variable for the Hugging Face token is set.\")\n",
    "    print(\"This might be overriding your login.\")\n",
    "else:\n",
    "    print(\"No Hugging Face token found in environment variables. That's good.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae080821",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment 1: LoRA Fine-Tuning with Llama-3-8B\n",
    "# This approach fine-tunes a small number of parameters (adapters) on top of a frozen Llama-3-8B model. It's memory and computationally efficient.\n",
    "# **Note:** The smallest available Llama-3 model is 8B. This requires authentication with Hugging Face.\n",
    "\n",
    "# --- Llama-3 LoRA Setup ---\n",
    "try:\n",
    "    from huggingface_hub import notebook_login\n",
    "    notebook_login()\n",
    "except ImportError:\n",
    "    print(\"Please run `pip install huggingface_hub` and log in to use gated models like Llama-3.\")\n",
    "\n",
    "# --- Config ---\n",
    "MODEL_CHECKPOINT_LLAMA = \"meta-llama/Meta-Llama-3-8B\"\n",
    "OUTPUT_DIR_LLAMA = \"./big5_classifier_llama_lora\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ac93d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model & Tokenizer ---\n",
    "MODEL_CHECKPOINT_LLAMA = \"roberta-small\"  #\"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "tokenizer_llama = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT_LLAMA)\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer_llama.pad_token is None:tokenizer_llama.pad_token = tokenizer_llama.eos_token\n",
    "\n",
    "model_llama_lora = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_CHECKPOINT_LLAMA,\n",
    "    num_labels=2,\n",
    "    torch_dtype=torch.bfloat16, # Use bfloat16 for memory efficiency\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "model_llama_lora.config.pad_token_id = tokenizer_llama.pad_token_id\n",
    "\n",
    "# --- LoRA Config ---\n",
    "lora_config_llama = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"] # Target modules for Llama\n",
    ")\n",
    "model_llama_lora = get_peft_model(model_llama_lora, lora_config_llama)\n",
    "model_llama_lora.print_trainable_parameters()\n",
    "\n",
    "# --- Tokenization ---\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer_llama(examples[\"text\"], truncation=True, max_length=256)\n",
    "\n",
    "tokenized_dataset_llama = main_dataset_dict.map(tokenize_function, batched=True).remove_columns([\"text\"])\n",
    "\n",
    "# --- Trainer ---\n",
    "trainer_llama = Trainer(\n",
    "    model=model_llama_lora,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR_LLAMA,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,  # Lower batch size for large models\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset_llama[\"train\"],\n",
    "    eval_dataset=tokenized_dataset_llama[\"validation\"],\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer_llama),\n",
    "    compute_metrics=lambda p: {\"accuracy\": accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1))},\n",
    ")\n",
    "\n",
    "# --- Training ---\n",
    "print(\"\\nStarting Llama-3 LoRA fine-tuning...\")\n",
    "trainer_llama.train() # Uncomment to run training\n",
    "print(\"Llama-3 LoRA training would be run here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd623797",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment 2: Full Fine-Tuning with RoBERTa-Large\n",
    "# This is the traditional approach where all 355 million parameters of RoBERTa-Large are updated during training. It can be very effective but requires more computational resources.\n",
    "# --- RoBERTa Full Fine-Tune Setup ---\n",
    "MODEL_CHECKPOINT_ROBERTA = \"roberta-large\"\n",
    "OUTPUT_DIR_ROBERTA = \"./big5_classifier_roberta_full\"\n",
    "\n",
    "# --- Model & Tokenizer ---\n",
    "tokenizer_roberta = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT_ROBERTA)\n",
    "model_roberta_full = AutoModelForSequenceClassification.from_pretrained( MODEL_CHECKPOINT_ROBERTA, num_labels=2).to(device)\n",
    "print(f\"\\nRoBERTa-Large has {model_roberta_full.num_parameters():,} total parameters (all trainable).\")\n",
    "\n",
    "# --- Tokenization ---\n",
    "def tokenize_function_roberta(examples):\n",
    "    return tokenizer_roberta(examples[\"text\"], truncation=True, max_length=512)\n",
    "tokenized_dataset_roberta = main_dataset_dict.map(tokenize_function_roberta, batched=True).remove_columns([\"text\"])\n",
    "\n",
    "# --- Trainer ---\n",
    "trainer_roberta = Trainer(\n",
    "    model=model_roberta_full,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR_ROBERTA,\n",
    "        num_train_epochs=16,\n",
    "        per_device_train_batch_size=16, # Can be higher than Llama-3\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset_roberta[\"train\"],\n",
    "    eval_dataset=tokenized_dataset_roberta[\"validation\"],\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer_roberta),\n",
    "    compute_metrics=lambda p: {\"accuracy\": accuracy_score(p.label_ids, np.argmax(p.predictions, axis=1))},\n",
    ")\n",
    "# --- Training ---\n",
    "print(\"\\nStarting RoBERTa-Large full fine-tuning...\")\n",
    "trainer_roberta.train() # Uncomment to run training\n",
    "print(\"RoBERTa full fine-tuning would be run here.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9538b3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Experiment 3: Zero-Shot Classification with GPT-4o\n",
    "# This approach leverages a powerful, general-purpose model's existing knowledge. We simply ask it to classify the text based on a carefully crafted prompt, without any training data.\n",
    "# --- GPT-4o Zero-Shot Setup ---\n",
    "try:\n",
    "    openai.api_key = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "except Exception as e:\n",
    "    print(\"Could not set OpenAI API key.\", e)\n",
    "    \n",
    "def classify_with_gpt4o(text, trait):\n",
    "    \"\"\"Classifies a single text using GPT-4o with a zero-shot prompt.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a psychology expert. Read the following text and determine if it indicates the author has the personality trait of '{trait}'.\n",
    "    The trait '{trait}' is defined as being outgoing, talkative, and energetic.\n",
    "    Respond with only the word 'yes' or 'no'.\n",
    "\n",
    "    Text: \"{text}\"\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=3,\n",
    "            temperature=0,\n",
    "        )\n",
    "        answer = response.choices[0].message.content.strip().lower()\n",
    "        return 1 if 'yes' in answer else 0\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return -1 # Return -1 for errors\n",
    "\n",
    "# --- Evaluation ---\n",
    "print(\"\\nRunning GPT-4o zero-shot evaluation...\")\n",
    "# Note: This will make one API call per item in the test set. This can be slow and costly.\n",
    "# We will run on a small sample.\n",
    "sample_test_df = test_df.sample(n=10, random_state=42)\n",
    "predictions_gpt = [classify_with_gpt4o(text, TARGET_COLUMN) for text in sample_test_df['text']]\n",
    "true_labels_gpt = sample_test_df['label'].tolist()\n",
    "\n",
    "# Filter out errors\n",
    "valid_preds = [p for p, t in zip(predictions_gpt, true_labels_gpt) if p != -1]\n",
    "valid_labels = [t for p, t in zip(predictions_gpt, true_labels_gpt) if p != -1]\n",
    "\n",
    "if valid_labels:\n",
    "    accuracy_gpt = accuracy_score(valid_labels, valid_preds)\n",
    "    print(f\"GPT-4o Zero-Shot Accuracy on 10 samples: {accuracy_gpt:.4f}\")\n",
    "else:\n",
    "    print(\"Could not get any valid predictions from GPT-4o.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc3c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment 4: SVM with Simulated LIWC Features\n",
    "# This is a classic NLP approach. Instead of deep learning, we engineer features using a lexicon (like LIWC) and train a simple, powerful classifier like an SVM.\n",
    "# **Note:** LIWC is proprietary. We simulate its output by creating a simple word-counting function.\n",
    "# --- SVM + LIWC Setup ---\n",
    "def simulate_liwc_features(text):\n",
    "    \"\"\"\n",
    "    A simplified simulation of LIWC. Counts words from predefined categories.\n",
    "    In a real scenario, you'd use the official LIWC tool/lexicon.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    features = {\n",
    "        'pronoun_i': len(re.findall(r'\\b(i|me|my|mine)\\b', text)),\n",
    "        'positive_affect': len(re.findall(r'\\b(love|nice|sweet|happy|good)\\b', text)),\n",
    "        'negative_affect': len(re.findall(r'\\b(hate|sad|angry|bad|awful)\\b', text)),\n",
    "        'social': len(re.findall(r'\\b(friend|party|talk|we|us|our)\\b', text)),\n",
    "        'work': len(re.findall(r'\\b(work|job|office|company)\\b', text)),\n",
    "    }\n",
    "    return features\n",
    "\n",
    "# --- Feature Extraction ---\n",
    "import re # Needed for the LIWC simulation\n",
    "\n",
    "print(\"\\nExtracting simulated LIWC features...\")\n",
    "# Convert text data to a list of feature dicts\n",
    "train_features = [simulate_liwc_features(text) for text in train_df['text']]\n",
    "test_features = [simulate_liwc_features(text) for text in test_df['text']]\n",
    "\n",
    "# Convert dicts to DataFrame\n",
    "train_features_df = pd.DataFrame(train_features)\n",
    "test_features_df = pd.DataFrame(test_features)\n",
    "\n",
    "# Get labels\n",
    "y_train = train_df['label']\n",
    "y_test = test_df['label']\n",
    "\n",
    "# --- SVM Training ---\n",
    "print(\"Training SVM model...\")\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(train_features_df, y_train)\n",
    "\n",
    "# --- Evaluation ---\n",
    "predictions_svm = svm_model.predict(test_features_df)\n",
    "accuracy_svm = accuracy_score(y_test, predictions_svm)\n",
    "print(f\"SVM + LIWC Accuracy: {accuracy_svm:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
